--source include/have_util_zstd.inc

--let $tmp_dir=`SELECT @@GLOBAL.secure_file_priv`

set enable_sql_wsenv=1;
#
# create table with highly repeating data
# and dump it compressed
# the result file should be small
#

CREATE TABLE t1(x VARCHAR(1000));
INSERT INTO t1 VALUES (REPEAT("xyz", 100));
INSERT INTO t1 SELECT t1.* FROM t1, t1 t12, t1 t13, t1 t14;
INSERT INTO t1 SELECT t1.* FROM t1, t1 t12, t1 t13, t1 t14;
INSERT INTO t1 SELECT t1.* FROM t1, t1 t12, t1 t13, t1 t14;

# dump data into file
--let $ws_output_file=$SQL_WSENV_MTR_PATH/t1.txt
--replace_result $ws_output_file OUTPUT_FILE
--eval SELECT * FROM t1 INTO OUTFILE '$ws_output_file';
# dump data into file, compressed
--replace_result $ws_output_file OUTPUT_FILE
--eval SELECT * FROM t1 INTO OUTFILE '$ws_output_file' COMPRESSED;

# validate compressed output
--let output_file=$tmp_dir/t1.txt
--exec $MTR_WSENV_COPYTOLOCAL_CMD $ws_output_file $output_file
--let output_file_zstd=$tmp_dir/t1.txt.0.zst
--let $ws_output_file_zstd=$SQL_WSENV_MTR_PATH/t1.txt.0.zst
--exec $MTR_WSENV_COPYTOLOCAL_CMD $ws_output_file_zstd $output_file_zstd
--exec zstd -dcq $output_file_zstd | diff $output_file -

#
# create table with 100000 rows of random data with 80 byte rows
# and dump it compressed
# the result should be low compressed
#

TRUNCATE TABLE t1;
# generate random input data for low compression
--exec dd bs=10000000 count=1 if=/dev/urandom | base64 -w 80 > $tmp_dir/t1.tmp
--let $ws_input_file=$SQL_WSENV_MTR_PATH/t2.txt
--exec head -n 100000  $tmp_dir/t1.tmp > $tmp_dir/t2.txt
--exec $MTR_WSENV_COPYFROMLOCAL_CMD $tmp_dir/t2.txt $ws_input_file
--remove_file $tmp_dir/t1.tmp
--remove_file $tmp_dir/t2.txt
# load input data
--replace_result $ws_input_file INPUT_FILE
--eval LOAD DATA INFILE '$ws_input_file' INTO TABLE t1;
# dump random data into file, compressed
--let $ws_output_file2=$SQL_WSENV_MTR_PATH/t2.txt
--let $ws_output_file_zstd2=$SQL_WSENV_MTR_PATH/t2.txt.0.zst
--replace_result $ws_output_file2 OUTPUT_FILE2
--eval SELECT * FROM t1 INTO OUTFILE '$ws_output_file2' COMPRESSED(0);
# validate
--let input_file=$tmp_dir/t2.txt
--exec $MTR_WSENV_COPYTOLOCAL_CMD $ws_input_file $input_file
--let output_file_zstd2=$tmp_dir/t2.txt.0.zst
--exec $MTR_WSENV_COPYTOLOCAL_CMD $ws_output_file_zstd2 $output_file_zstd2
--exec zstd -dcq $output_file_zstd2 | diff $input_file -


#
# check how compression works for empty table
#

TRUNCATE TABLE t1;
# dump empty table
--let $ws_output_file3=$SQL_WSENV_MTR_PATH/t3.txt
--replace_result $ws_output_file3 OUTPUT_FILE3
--eval SELECT * FROM t1 INTO OUTFILE '$ws_output_file3' COMPRESSED;
# validate
--let $ws_output_file_zstd3=$SQL_WSENV_MTR_PATH/t3.txt.0.zst
--let output_file_zstd3=$tmp_dir/t3.txt.0.zst
--exec $MTR_WSENV_COPYTOLOCAL_CMD $ws_output_file_zstd3 $output_file_zstd3
--exec zstd -dcq $output_file_zstd3 | wc -l


#
# test how compression works for big rows (5000 bytes)
# which do not fit into internal cache buffer
# and require more than one write operation for a single row
#

CREATE TABLE t10(x varchar(5001));
# generate rows bigger 4K, so they cannot be flushed in one step
--exec dd bs=10000000 count=1 if=/dev/urandom | base64 -w 5000 > $tmp_dir/t1.tmp
--let $input_file_big_row=$tmp_dir/t10.txt
--exec head -n 1000  $tmp_dir/t1.tmp > $input_file_big_row
--remove_file $tmp_dir/t1.tmp
#
--let  $ws_input_file_big_row=$SQL_WSENV_MTR_PATH/t10.txt
--exec $MTR_WSENV_COPYFROMLOCAL_CMD $input_file_big_row $ws_input_file_big_row
#
# load input data
--replace_result $ws_input_file_big_row INPUT_FILE_BIG_ROW
--eval LOAD DATA INFILE '$ws_input_file_big_row' INTO TABLE t10;
# dump data into file, compressed
--let $output_file_big_row=$SQL_WSENV_MTR_PATH/t10.txt
--replace_result $output_file_big_row OUTPUT_FILE_BIG_ROW
--eval SELECT * FROM t10 INTO OUTFILE '$output_file_big_row' COMPRESSED;
# validate
--let $output_file_zstd_big_row=$tmp_dir/t10.txt.0.zst
--exec $MTR_WSENV_COPYTOLOCAL_CMD $SQL_WSENV_MTR_PATH/t10.txt.0.zst $output_file_zstd_big_row
--exec zstd -dcq $output_file_zstd_big_row | diff $input_file_big_row -

set enable_sql_wsenv=0;

# cleanup
--remove_file $output_file
--remove_file $output_file_zstd
--remove_file $input_file
--remove_file $output_file_zstd2
--remove_file $output_file_zstd3
--remove_file $input_file_big_row
--remove_file $output_file_zstd_big_row

DROP TABLE t1, t10;
